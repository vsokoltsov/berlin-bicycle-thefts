{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565706bd",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902a6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import geopandas as gpd\n",
    "import libpysal as lps\n",
    "import re\n",
    "from shapely import wkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50955b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(os.path.abspath('')).resolve().parents[0]\n",
    "DATA = os.path.join(ROOT, \"data\")\n",
    "EXTERNAL_DATA = os.path.join(DATA, \"external\") \n",
    "INTERIM_DATA = os.path.join(DATA, \"interim\")\n",
    "RAW_DATA = os.path.join(DATA, \"raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c9c5d",
   "metadata": {},
   "source": [
    "## Read dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312380b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_parquet(\n",
    "    os.path.join(INTERIM_DATA, 'df_geo_etl.geoparquet.gzip')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71faaa23",
   "metadata": {},
   "source": [
    "## Base features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825b35e",
   "metadata": {},
   "source": [
    "### Number of thefts per day in LOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c59e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lor\"] = df[\"lor\"].astype(str).str.zfill(8)\n",
    "df[\"date\"] = pd.to_datetime(df[\"start_date\"]).dt.normalize()\n",
    "df['y_count'] = df.groupby([\"date\", \"lor\"])[\"lor\"].transform(\"size\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479fc7c9",
   "metadata": {},
   "source": [
    "### Exposure offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951cd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_day_lor = df.groupby([\"lor\",\"date\"])[\"population_total\"].transform(\"first\")\n",
    "df[\"offset_log_pop\"] = np.log(pop_day_lor.clip(lower=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbc5e3",
   "metadata": {},
   "source": [
    "### Rate previous mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5ba488",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cnt_by_day = df.groupby([\"lor\",\"date\"]).size().sort_index()\n",
    "\n",
    "grp = y_cnt_by_day.groupby(level=0)\n",
    "cum_sum = grp.cumsum()\n",
    "cum_cnt = grp.cumcount() + 1\n",
    "\n",
    "y_mean_prev_by_day = (cum_sum - y_cnt_by_day) / (cum_cnt - 1)\n",
    "y_mean_prev_by_day = y_mean_prev_by_day.where(cum_cnt > 1, np.nan).fillna(0.0)\n",
    "\n",
    "mi = pd.MultiIndex.from_frame(df[[\"lor\",\"date\"]])\n",
    "df[\"y_mean_prev\"] = y_mean_prev_by_day.reindex(mi).to_numpy()\n",
    "df[\"rate_prev_mean\"] = (df[\"y_mean_prev\"] / pop_day_lor).fillna(0.0)\n",
    "df[\"rate_prev_mean_1k\"] = 1000.0 * df[\"rate_prev_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793c77d",
   "metadata": {},
   "source": [
    "## Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4dc5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dow\"]         = df[\"date\"].dt.dayofweek\n",
    "df[\"is_weekend\"]  = (df[\"dow\"] >= 5).astype(int)\n",
    "df[\"month\"]       = df[\"date\"].dt.month\n",
    "df[\"weekofyear\"]  = df[\"date\"].dt.isocalendar().week.astype(int)\n",
    "years = sorted(df[\"date\"].dt.year.unique())\n",
    "de_be = holidays.Germany(prov=\"BE\", years=years)\n",
    "df[\"is_holiday_BE\"] = df[\"date\"].dt.date.map(lambda d: int(d in de_be))\n",
    "doy = df[\"date\"].dt.dayofyear.astype(float)\n",
    "df[\"sin_doy\"] = np.sin(2 * np.pi * doy / 365.25)\n",
    "df[\"cos_doy\"] = np.cos(2 * np.pi * doy / 365.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef8ffe",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6370f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feat_one_lor(s: pd.Series) -> dict[str, pd.Series]:\n",
    "    s = s.sort_index()\n",
    "\n",
    "    # Calender index for all values of this LOR\n",
    "    full_idx = pd.date_range(s.index.min(), s.index.max(), freq=\"D\")\n",
    "    s_full = s.reindex(full_idx, fill_value=0)\n",
    "\n",
    "    # Lags for last days\n",
    "    y_lag1  = s_full.shift(1)\n",
    "    y_lag7  = s_full.shift(7)\n",
    "\n",
    "    # Rolling mean for previous days without today\n",
    "    y_roll7  = s_full.shift(1).rolling(7,  min_periods=1).mean()\n",
    "\n",
    "    # Difference between 'today' and 'yesterday'\n",
    "    y_diff1 = s_full.diff(1)\n",
    "\n",
    "    # zero_streak:  Length of consequent 0 (including today)\n",
    "    arr = s_full.to_numpy()\n",
    "    zs = np.zeros_like(arr, dtype=int)\n",
    "    run = 0\n",
    "    for i, v in enumerate(arr):\n",
    "        if v == 0:\n",
    "            run += 1\n",
    "        else:\n",
    "            run = 0\n",
    "        zs[i] = run\n",
    "    zero_streak = pd.Series(zs, index=s_full.index)\n",
    "\n",
    "    # Values for observing dates only\n",
    "    out = {\n",
    "        \"y_lag1\":  y_lag1.reindex(s.index),\n",
    "        \"y_lag7\":  y_lag7.reindex(s.index),\n",
    "        \"y_roll7\":  y_roll7.reindex(s.index),\n",
    "        \"y_diff1\":  y_diff1.reindex(s.index),\n",
    "        \"zero_streak\": zero_streak.reindex(s.index),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def _roll7_past(s: pd.Series) -> pd.Series:\n",
    "    full_idx = pd.date_range(s.index.min(), s.index.max(), freq=\"D\")\n",
    "    s_full = s.reindex(full_idx, fill_value=0)\n",
    "    y_roll7 = s_full.shift(1).rolling(7, min_periods=1).mean()\n",
    "    return y_roll7.reindex(s.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bece7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_df = pd.MultiIndex.from_frame(df[[\"lor\",\"date\"]])\n",
    "\n",
    "acc = {k: [] for k in [\"y_lag1\",\"y_lag7\",\"y_roll7\",\"y_diff1\",\"zero_streak\"]}\n",
    "\n",
    "# Loop through all LORs without external dataframes\n",
    "for lor, s in y_cnt_by_day.groupby(level=0):\n",
    "    s_lor = s.droplevel(0)\n",
    "    feats = _feat_one_lor(s_lor)\n",
    "    for k, ser in feats.items():\n",
    "        # MultiIndex (lor, date) for DataFrame mapping\n",
    "        ser.index = pd.MultiIndex.from_product([[lor], ser.index], names=[\"lor\",\"date\"])\n",
    "        acc[k].append(ser)\n",
    "\n",
    "# Concat for each feature and map to given df\n",
    "for k, parts in acc.items():\n",
    "    if parts:\n",
    "        feat_series = pd.concat(parts).sort_index()\n",
    "        df[k] = feat_series.reindex(mi_df).to_numpy()\n",
    "    else:\n",
    "        df[k] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26377c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pop_density_km2\"] = (df[\"population_total\"] / df[\"area_km2\"]).replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b031e",
   "metadata": {},
   "source": [
    "## Dimension / static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06cb9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lor_polys = (df[[\"lor\",\"geometry\"]]\n",
    "            .dropna(subset=[\"geometry\"])\n",
    "            .drop_duplicates(subset=[\"lor\"])\n",
    "            .copy())\n",
    "\n",
    "# Convert `geometry` field to Shapely\n",
    "geom_raw = lor_polys[\"geometry\"]\n",
    "first = geom_raw.iloc[0]\n",
    "\n",
    "if isinstance(first, (bytes, bytearray, memoryview)):\n",
    "    # WKB bytes\n",
    "    geom = gpd.GeoSeries.from_wkb(geom_raw)\n",
    "elif isinstance(first, str):\n",
    "    try:\n",
    "        geom = gpd.GeoSeries.from_wkt(geom_raw)\n",
    "    except Exception:\n",
    "        geom = gpd.GeoSeries.from_wkb(geom_raw.apply(bytes.fromhex))\n",
    "else:\n",
    "    # Shapely\n",
    "    geom = gpd.GeoSeries(geom_raw)\n",
    "gpol = gpd.GeoDataFrame(lor_polys[[\"lor\"]], geometry=geom)\n",
    "\n",
    "# Set CRS\n",
    "if gpol.crs is None:\n",
    "    minx, miny, maxx, maxy = gpol.total_bounds\n",
    "    # if coordinates like longitude / latitude\n",
    "    if max(abs(minx),abs(maxx)) <= 180 and max(abs(miny),abs(maxy)) <= 90:\n",
    "        gpol = gpol.set_crs(4326)\n",
    "    else:\n",
    "        gpol = gpol.set_crs(25833)\n",
    "\n",
    "gpol = gpol.to_crs(25833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef362e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/8r5h4ym13fb_mzmqc1hfkt580000gn/T/ipykernel_1421/1762662328.py:26: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
      "  W = lps.weights.Queen.from_dataframe(gpol)\n"
     ]
    }
   ],
   "source": [
    "gpol = (df[[\"lor\",\"geometry\"]]\n",
    "        .dropna(subset=[\"geometry\"])\n",
    "        .drop_duplicates(subset=[\"lor\"])\n",
    "        .copy())\n",
    "\n",
    "gpol[\"lor\"] = gpol[\"lor\"].astype(str).str.zfill(8)\n",
    "\n",
    "first_geom = gpol[\"geometry\"].iloc[0]\n",
    "if isinstance(first_geom, (bytes, bytearray, memoryview)):\n",
    "    gpol[\"geometry\"] = gpol[\"geometry\"].apply(lambda b: wkb.loads(b))\n",
    "\n",
    "gpol = gpd.GeoDataFrame(gpol, geometry=\"geometry\")\n",
    "\n",
    "if gpol.crs is None:\n",
    "    minx, miny, maxx, maxy = gpol.total_bounds\n",
    "    gpol = gpol.set_crs(4326 if max(abs(minx),abs(maxx))<=180 and max(abs(miny),abs(maxy))<=90 else 25833)\n",
    "gpol = gpol.to_crs(25833)\n",
    "\n",
    "# Set index to 'lor'\n",
    "if \"lor\" in gpol.columns:\n",
    "    gpol = gpol.set_index(\"lor\", drop=True)\n",
    "else:\n",
    "    gpol.index.name = \"lor\"\n",
    "\n",
    "# Queen weights for the neighbours\n",
    "W = lps.weights.Queen.from_dataframe(gpol)\n",
    "W.transform = \"r\"\n",
    "id_order = W.id_order \n",
    "\n",
    "df[\"lor\"]  = df[\"lor\"].astype(str).str.zfill(8)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"] if \"date\" in df.columns else df[\"start_date\"]).dt.normalize()\n",
    "\n",
    "y_cnt_by_day = df.groupby([\"lor\",\"date\"]).size().sort_index()  # MultiIndex Series\n",
    "\n",
    "#  7-day past mean for each LOR\n",
    "y_roll7_by_day = (y_cnt_by_day\n",
    "                    .groupby(level=0)\n",
    "                    .apply(lambda s: _roll7_past(s.droplevel(0)))\n",
    "                    .rename(\"y_roll7\"))\n",
    "y_roll7_by_day.index = y_roll7_by_day.index.set_names([\"lor\",\"date\"])\n",
    "\n",
    "# Spatial lag by days: W * y_roll7_vector\n",
    "parts = []\n",
    "for dt, s_day in y_roll7_by_day.groupby(level=1):\n",
    "    # s_day: index = (lor, date)\n",
    "    s_lor = s_day.droplevel(1)\n",
    "    # Fill missing values with 0.0\n",
    "    v = s_lor.reindex(id_order).fillna(0.0).to_numpy()\n",
    "    neigh = W.sparse.dot(v).ravel()\n",
    "    mi = pd.MultiIndex.from_product([id_order, [dt]], names=[\"lor\",\"date\"])\n",
    "    parts.append(pd.Series(neigh, index=mi))\n",
    "\n",
    "spatial_lag_series = pd.concat(parts).rename(\"spatial_lag_y_roll7\")\n",
    "\n",
    "# Add value back by (lor, date)\n",
    "mi_df = pd.MultiIndex.from_frame(df[[\"lor\",\"date\"]])\n",
    "df[\"spatial_lag_y_roll7\"] = spatial_lag_series.reindex(mi_df).to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238f9f7",
   "metadata": {},
   "source": [
    "## Events' content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663b4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkey = [df[\"lor\"], df[\"date\"]]  # Group by lor and date\n",
    "\n",
    "# attempt_rate — number of attempts (0/1) per day in LOR\n",
    "attempt_bin = pd.to_numeric(df[\"attempt\"], errors=\"coerce\").fillna(0).clip(0, 1)\n",
    "df[\"attempt_rate\"] = attempt_bin.groupby(gkey).transform(\"mean\")\n",
    "\n",
    "# Share top-k bicycle types per day for LOR\n",
    "k = 5\n",
    "top_types = (df[\"bicycle_type\"].astype(str).fillna(\"Unknown\")\n",
    "            .value_counts().head(k).index.tolist())\n",
    "\n",
    "share_cols = []\n",
    "for t in top_types:\n",
    "    slug = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", str(t)).strip(\"_\").lower()\n",
    "    col = f\"share_bt_{slug}\"\n",
    "    share_cols.append(col)\n",
    "    df[col] = (df[\"bicycle_type\"].astype(str).eq(t).astype(int)).groupby(gkey).transform(\"mean\")\n",
    "\n",
    "# Share \"other\" like `1 - sum(top-k)`\n",
    "df[\"share_bt_other\"] = (1.0 - df[share_cols].sum(axis=1)).clip(lower=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdbc7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "_order = df.index\n",
    "df = df.sort_values([\"lor\",\"date\"], kind=\"stable\").copy()\n",
    "\n",
    "# First row in (lor, date) with daily feature value\n",
    "is_first_ld = df.groupby([\"lor\",\"date\"]).cumcount().eq(0)\n",
    "\n",
    "for base_col in [\"attempt_rate\"] + share_cols + [\"share_bt_other\"]:\n",
    "    tmp = np.where(is_first_ld, df[base_col], np.nan)\n",
    "    # mean for last 7 days (no current date) within LOR\n",
    "    rolled = (pd.Series(tmp)\n",
    "            .groupby(df[\"lor\"])\n",
    "            .transform(lambda s: s.shift(1).rolling(7, min_periods=1).mean()))\n",
    "    out_col = base_col + \"_roll7_prev\"\n",
    "    df[out_col] = rolled\n",
    "    # Set this value for all rows for this day per LOR\n",
    "    df[out_col] = df.groupby([\"lor\",\"date\"])[out_col].transform(\"max\")\n",
    "\n",
    "# Set initial order\n",
    "df = df.loc[_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428aef8",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f88a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_days  = 60\n",
    "test_days = 60\n",
    "gap_days  = 0\n",
    "date_col  = \"date\" if \"date\" in df.columns else \"start_date\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f97055",
   "metadata": {},
   "source": [
    "### Calculate time borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd480a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "d    = pd.to_datetime(df[date_col]).dt.normalize()\n",
    "last = d.max()\n",
    "gap  = pd.Timedelta(days=gap_days)\n",
    "\n",
    "test_start = last - pd.Timedelta(days=test_days) + pd.Timedelta(days=1)\n",
    "val_end    = test_start - pd.Timedelta(days=1) - gap\n",
    "val_start  = val_end   - pd.Timedelta(days=val_days) + pd.Timedelta(days=1)\n",
    "train_end  = val_start - pd.Timedelta(days=1) - gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50b697",
   "metadata": {},
   "source": [
    "### Build masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a7e6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = d <= train_end\n",
    "m_val   = (d >= val_start) & (d <= val_end)\n",
    "m_test  = d >= test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d08d8",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c84912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[m_train].copy()\n",
    "df_val   = df.loc[m_val].copy()\n",
    "df_test  = df.loc[m_test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef99c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_train, df_val, df_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4528005",
   "metadata": {},
   "source": [
    "## Weather hot / cold attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6732e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(dataframes):\n",
    "\n",
    "    # Daily weather with no duplicates: one row per date\n",
    "    wx_cols = [\"temperature_2m_mean\",\"precipitation_sum\",\"wind_speed_10m_max\",\"sunshine_h\"]\n",
    "    w = (df[[\"date\"] + wx_cols]\n",
    "        .dropna(subset=[\"date\"])\n",
    "        .drop_duplicates(subset=[\"date\"])\n",
    "        .sort_values(\"date\")\n",
    "        .set_index(\"date\"))\n",
    "\n",
    "    # Precipitation: 1 day lag + 3 prev days sum (exclude current)\n",
    "    w[\"precip_sum_lag1\"]  = w[\"precipitation_sum\"].shift(1)\n",
    "    w[\"precip_sum_roll3\"] = w[\"precipitation_sum\"].shift(1).rolling(3, min_periods=1).sum()\n",
    "\n",
    "    # Temperature: mean for the prev day\n",
    "    w[\"t_mean_lag1\"]      = w[\"temperature_2m_mean\"].shift(1).mean()\n",
    "\n",
    "    # Sum: average for last 7 days\n",
    "    w[\"sunshine_roll7\"]   = w[\"sunshine_h\"].shift(1).rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # Rain day flag\n",
    "    w[\"is_rainy\"] = (w[\"precipitation_sum\"] > 1.0).astype(int)\n",
    "\n",
    "    # Temperature quantiles\n",
    "    q_hot  = w[\"temperature_2m_mean\"].quantile(0.80)\n",
    "    q_cold = w[\"temperature_2m_mean\"].quantile(0.20)\n",
    "\n",
    "    w[\"is_hot\"]  = (w[\"temperature_2m_mean\"] >= q_hot).astype(int)\n",
    "    w[\"is_cold\"] = (w[\"temperature_2m_mean\"] <= q_cold).astype(int)\n",
    "\n",
    "    # Prepare daily weather df\n",
    "    daily_weather = w.reset_index()[[\n",
    "        \"date\",\n",
    "        \"precip_sum_lag1\",\"precip_sum_roll3\",\"t_mean_lag1\",\"sunshine_roll7\",\n",
    "        \"is_rainy\",\"is_hot\",\"is_cold\"\n",
    "    ]]\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"start_date\"]).dt.normalize()\n",
    "\n",
    "    # Delete previouse version of columns\n",
    "    cols_to_drop = [\"precip_sum_lag1\",\"precip_sum_roll3\",\"t_mean_lag1\",\"sunshine_roll7\",\n",
    "                    \"is_rainy\",\"is_hot\",\"is_cold\"]\n",
    "    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    # Merge two dataframes by date\n",
    "    df = df.merge(daily_weather, on=\"date\", how=\"left\")\n",
    "\n",
    "    dataframes[idx] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99c6c5",
   "metadata": {},
   "source": [
    "### Events attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e223993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(dataframes):\n",
    "    # price_median_clip — median price for 99-percentile\n",
    "    price_num = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    p99 = price_num.quantile(0.99)\n",
    "    df[\"price_median_clip\"] = price_num.clip(upper=p99).groupby(gkey).transform(\"median\")\n",
    "\n",
    "    _order = df.index\n",
    "    df = df.sort_values([\"lor\",\"date\"], kind=\"stable\").copy()\n",
    "\n",
    "    # First row in (lor, date) with daily feature value\n",
    "    is_first_ld = df.groupby([\"lor\",\"date\"]).cumcount().eq(0)\n",
    "\n",
    "    for base_col in [\"price_median_clip\"]:\n",
    "        tmp = np.where(is_first_ld, df[base_col], np.nan)\n",
    "        # mean for last 7 days (no current date) within LOR\n",
    "        rolled = (pd.Series(tmp)\n",
    "                .groupby(df[\"lor\"])\n",
    "                .transform(lambda s: s.shift(1).rolling(7, min_periods=1).mean()))\n",
    "        out_col = base_col + \"_roll7_prev\"\n",
    "        df[out_col] = rolled\n",
    "        # Set this value for all rows for this day per LOR\n",
    "        df[out_col] = df.groupby([\"lor\",\"date\"])[out_col].transform(\"max\")\n",
    "\n",
    "    # Set initial order\n",
    "    df = df.loc[_order]\n",
    "\n",
    "    dataframes[idx] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca234f5",
   "metadata": {},
   "source": [
    "## Features for usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f0f5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_glm = [\n",
    "    \"lor\",\n",
    "    \"dow\",\n",
    "    \"is_weekend\",\n",
    "    \"weekofyear\",\n",
    "    \"sin_doy\",\n",
    "    \"cos_doy\",\n",
    "    \"is_holiday_BE\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"precipitation_sum\",\n",
    "    \"wind_speed_10m_max\",\n",
    "    \"sunshine_h\",\n",
    "    \"precip_sum_lag1\",\n",
    "    \"precip_sum_roll3\",\n",
    "    \"t_mean_lag1\",\n",
    "    \"sunshine_roll7\",\n",
    "    \"is_rainy\",\n",
    "    \"is_hot\",\n",
    "    \"is_cold\",\n",
    "    \"y_lag1\",\n",
    "    \"y_lag7\",\n",
    "    \"y_roll7\",\n",
    "    \"y_diff1\",\n",
    "    \"zero_streak\",\n",
    "    \"rate_prev_mean_1k\",\n",
    "    \"pop_density_km2\",\n",
    "    \"poi_density_km2\",\n",
    "    \"spatial_lag_y_roll7\",\n",
    "    \"attempt_rate_roll7_prev\",\n",
    "    \"price_median_clip_roll7_prev\",\n",
    "    \"share_bt_diamond_frame_roll7_prev\",\n",
    "    \"share_bt_step_through_roll7_prev\",\n",
    "    \"share_bt_generic_roll7_prev\",\n",
    "    \"share_bt_kids_roll7_prev\",\n",
    "    \"share_bt_mtb_roll7_prev\",\n",
    "    \"share_bt_other_roll7_prev\"\n",
    "]\n",
    "\n",
    "offset_col = \"offset_log_pop\"\n",
    "\n",
    "feature_columns = feature_cols_glm + [\"offset_log_pop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570359bb",
   "metadata": {},
   "source": [
    "## Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6ae8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(\n",
    "    os.path.join(INTERIM_DATA, 'df_train.geoparquet.gzip'),\n",
    "    compression='gzip'\n",
    ")\n",
    "df_val.to_parquet(\n",
    "    os.path.join(INTERIM_DATA, 'df_val.geoparquet.gzip'),\n",
    "    compression='gzip'\n",
    ")\n",
    "df_test.to_parquet(\n",
    "    os.path.join(INTERIM_DATA, 'df_test.geoparquet.gzip'),\n",
    "    compression='gzip'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Berlin bicycle theft",
   "language": "python",
   "name": "berlin-bicycle-theft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
